# سوالات متداول

### 1. چگونه یک مدل پایه را انتخاب کنیم؟

**gpt-3.5-turbo** •gpt-3.5-turbo یک نسخه ارتقا یافته از سری مدل‌های gpt-3 است. این مدل قدرتمندتر از gpt-3 است و می‌تواند وظایف پیچیده‌تری را انجام دهد. این مدل پیشرفت‌های قابل توجهی در درک متن طولانی و استدلال بین اسناد داشته است. gpt-3.5 turbo می‌تواند متن منسجم‌تر و متقاعد کننده‌تر تولید کند. همچنین پیشرفت‌های بزرگی در خلاصه سازی، ترجمه و نوشتن خلاقانه دارد. **خوب در: درک متن طولانی، استدلال بین اسناد، خلاصه سازی، ترجمه، نوشتن خلاقانه**

**gpt-4** •gpt-4 جدیدترین و قدرتمندترین مدل زبان Transformer است. این مدل تقریباً 200 میلیارد پارامتر از پیش آموزش دیده دارد، که آن را در همه وظایف زبانی به ویژه وظایفی که نیاز به درک عمیق و تولید پاسخ‌های طولانی و پیچیده دارند، به اوج می‌رساند. Gpt-4 می‌تواند همه جنبه‌های زبان انسان را شامل درک مفاهیم انتزاعی و استدلال بین صفحات را اداره کند. Gpt-4 اولین سیستم واقعی درک زبان عمومی است که می‌تواند هر وظیفه‌ای در زمینه پردازش زبان طبیعی در زمینه هوش مصنوعی را اداره کند. **خوب در: \*همه وظایف پردازش زبان طبیعی، درک زبان، تولید متن طولانی، استدلال بین اسناد، درک مفاهیم انتزاعی\***لطفا به: [https://platform.openai.com/docs/models/overview](https://platform.openai.com/docs/models/overview) مراجعه کنید

### 2. چرا توصیه می‌شود max\_tokens را کوچکتر تنظیم کنیم؟

زیرا در پردازش زبان طبیعی، خروجی‌های متن طولانی‌تر معمولاً به زمان محاسباتی بیشتر و منابع محاسباتی بیشتری نیاز دارند. بنابراین، محدود کردن طول متن خروجی می‌تواند تا حدی هزینه محاسباتی و زمان را کاهش دهد. به عنوان مثال، تنظیم: max\_tokens=500، به این معنی است که فقط 500 توکن اول متن خروجی در نظر گرفته می‌شود، و بخش‌هایی که از این طول تجاوز می‌کنند، حذف می‌شوند. هدف از انجام این کار این است که اطمینان حاصل شود که طول متن خروجی از محدوده قابل قبول LLM تجاوز نمی‌کند، در حالی که از منابع محاسباتی به طور کامل استفاده می‌شود تا راندمان مدل بهبود یابد. از طرف دیگر، اغلب محدود کردن max\_tokens می‌تواند طول دستورالعمل را افزایش دهد، مانند محدودیت gpt-3.5-turbo که 4097 توکن است، اگر max\_tokens=4000 را تنظیم کنید، فقط 97 توکن برای دستورالعمل باقی می‌ماند، و اگر از این مقدار تجاوز کند، خطایی گزارش می‌شود.

### 3. چگونه می‌توان داده‌های متن طولانی را در دانش به طور معقولی تقسیم کرد؟

در برخی از برنامه‌های پردازش زبان طبیعی، متن اغلب به پاراگراف‌ها یا جملات تقسیم می‌شود تا پردازش و درک بهتر اطلاعات معنایی و ساختاری در متن انجام شود. واحد حداقل تقسیم به وظیفه خاص و پیاده سازی فنی بستگی دارد. به عنوان مثال:

• برای وظایف طبقه بندی متن، متن معمولاً به جملات یا پاراگراف‌ها تقسیم می‌شود.

• برای وظایف ترجمه ماشینی، باید از جملات یا پاراگراف‌های کامل به عنوان واحدهای تقسیم استفاده کرد.

در نهایت، هنوز به آزمایش و ارزیابی برای تعیین مناسب‌ترین فناوری توگذاری و واحد تقسیم نیاز است. می‌توان عملکرد فناوری‌ها و واحدهای تقسیم مختلف را در مجموعه آزمون مقایسه کرد تا طرح بهینه را انتخاب کرد.

### 4. چه تابع فاصله‌ای هنگام دریافت بخش‌بندی دانش استفاده می‌کنیم؟

از [شباهت کسینوسی](https://en.wikipedia.org/wiki/Cosine_similarity) استفاده می‌کنیم. انتخاب تابع فاصله معمولاً بی‌اهمیت است. توگذاری‌های OpenAI به طول 1 نرمال می‌شوند، که به این معنی است:

•استفاده از حاصلضرب نقطه‌ای برای محاسبه شباهت کسینوسی می‌تواند کمی سریع‌تر باشد

•شباهت کسینوسی و فاصله اقلیدسی به همان رتبه‌بندی منجر خواهند شد

پس از نرمال شدن بردارهای توگذاری به طول 1، محاسبه شباهت کسینوسی بین دو بردار می‌تواند به حاصلضرب نقطه‌ای آن‌ها ساده شود. از آنجا که بردارهای نرمال شده طول 1 دارند، نتیجه حاصلضرب نقطه‌ای برابر با نتیجه شباهت کسینوسی است.

از آنجا که محاسبه حاصلضرب نقطه‌ای سریع‌تر از سایر معیارهای شباهت (مانند فاصله اقلیدسی) است، استفاده از بردارهای نرمال شده برای محاسبه حاصلضرب نقطه‌ای می‌تواند تا حدی راندمان محاسباتی را بهبود بخشد.

### 5. **هنگام پر کردن کلید OpenAI، خطای "اعتبارسنجی ناموفق: شما از سهمیه فعلی خود تجاوز کرده‌اید، لطفاً برنامه و جزئیات صورت حساب خود را بررسی کنید" رخ می‌دهد. علت این خطا چیست؟**

این خطا نشان می‌دهد که موجودی حساب کلید OpenAI تمام شده است. لطفاً حساب OpenAI خود را در openai.com شارژ کنید. برای اطلاعات بیشتر در مورد برنامه‌ها و صورت حساب آن‌ها، به [OpenAI](https://openai.com/pricing) مراجعه کنید.

### 6. هنگام استفاده از کلید OpenAI برای گفتگو در برنامه، یک پیام خطا به شرح زیر وجود دارد. **علت چیست؟**

خطای 1：

```JSON
سرور با خطای داخلی مواجه شد و نتوانست درخواست شما را کامل کند。سرور یا پرکار است یا خطایی در برنامه وجود دارد
```

خطای 1：

```JSON
محدودیت نرخ برای default-gpt-3.5-turboin سازمان org-wDrZCxxxxxxxxxissoZb در requestsper min.  محدودیت: 3 / min.  لطفا در 20 ثانیه دیگر دوباره امتحان کنید. اگر همچنان با مشکل مواجه هستید، از طریق مرکز راهنمایی در help.openai.com با ما تماس بگیرید.  لطفاً یک روش پرداخت به حساب خود اضافه کنید تا محدودیت نرخ خود را افزایش دهید. از https://platform.openai.com/account/billing برای اضافه کردن روش پرداخت دیدن کنید.
```

لطفاً بررسی کنید که آیا محدودیت نرخ تماس با رابط رسمی به دست آمده است یا خیر. برای اطلاعات بیشتر، به [مستندات رسمی](https://platform.openai.com/docs/guides/rate-limits) مراجعه کنید.

### 6. پس از استقرار محلی، Explore-Chat خطای "آرگومان درخواست ناشناخته ارائه شده: functions" را برمی‌گرداند. چگونه می‌توان این مشکل را حل کرد؟

ابتدا لطفاً بررسی کنید که نسخه‌های فرانت‌اند و بک‌اند به روز و با یکدیگر سازگار هستند. این خطا همچنین می‌تواند در صورتی رخ دهد که از کلید Azure OpenAI بدون استقرار موفقیت‌آمیز مدل استفاده شود. تأیید کنید که منبع Azure OpenAI یک مدل مستقر دارد - نسخه مدل gpt-3.5-turbo باید 0613 یا جدیدتر باشد، زیرا نسخه‌های قبلی از قابلیت‌های تماس با تابع مورد نیاز Explore-Chat پشتیبانی نمی‌کنند.

### 7. هنگام تعویض مدل در برنامه، خطای زیر مشاهده می‌شود:

```JSON
Anthropic: کد خطا: 400 - f'error': f'type': "خطای درخواست نامعتبر, 'message': 'temperature: range: -1 or 0..1)
```

این خطا به این دلیل رخ می‌دهد که هر مدل دارای محدوده‌های معتبر مختلفی برای پارامترهای خود است. مطمئن شوید که مقدار پارامتر را با توجه به محدوده مجاز برای مدل فعلی پیکربندی کرده‌اید.

### 8. چگونه می‌توان پیام خطای زیر را حل کرد؟

```JSON
پرس و جو یا دستورالعمل پیشوند خیلی طولانی است، می‌توانید دستورالعمل پیشوند را کاهش دهید، یا max token را کوچک کنید، یا به یک LLM با اندازه محدودیت توکن بزرگتر تغییر دهید
```

می‌توانید مقدار "Max token" را در تنظیمات پارامتر Prompt Eng پایین بیاورید.

### 9. مدل‌های پیش‌فرض در Dify کدامند، و آیا می‌توان از LLMهای منبع باز استفاده کرد؟

A: مدل‌های پیش‌فرض را می‌توان در **تنظیمات - ارائه دهنده مدل** پیکربندی کرد. LLMهای تولید متن که در حال حاضر پشتیبانی می‌شوند شامل OpenAI، Azure OpenAl، Anthropic و غیره هستند. در عین حال، می‌توان LLMهای منبع باز میزبانی شده در Hugging Face، Replicate، xinference و غیره را نیز ادغام کرد.

### 10. دانش در نسخه Community Edition  هنگامی که حالت بخش‌بندی Q\&A فعال است، در "در صف انتظار" گیر می‌کند.

لطفاً بررسی کنید که آیا محدودیت نرخ برای کلید API مدل توگذاری استفاده شده به دست آمده است یا خیر.

### 11. خطای "توکن نامعتبر" هنگام استفاده از برنامه ظاهر می‌شود.

اگر خطای "توکن نامعتبر" ظاهر شد، دو راه حل احتمالی وجود دارد:

* حافظه نهان مرورگر (کوکی‌ها، ذخیره‌سازی جلسه و ذخیره‌سازی محلی) یا حافظه نهان برنامه در موبایل را پاک کنید. سپس، دوباره به برنامه مراجعه کنید.
* URL برنامه را مجدداً تولید کنید و دوباره با URL جدید به برنامه دسترسی پیدا کنید. این مشکل "توکن نامعتبر" را برطرف می‌کند.

### 12. محدودیت اندازه برای آپلود اسناد دانش چیست؟

حداکثر اندازه برای آپلود یک سند منفرد در حال حاضر 15 مگابایت است. همچنین محدودیت 100 سند در کل وجود دارد. اگر از استقرار محلی استفاده می‌کنید، این محدودیت‌ها را می‌توان تنظیم کرد. برای جزئیات در مورد تغییر محدودیت‌ها، به [مستندات](../../getting-started/install-self-hosted/install-faq.md#11.-how-to-solve-the-size-and-quantity-limitations-for-uploading-dataset-documents-in-the-local-depl) مراجعه کنید.

### 13. چرا Claude هنوز هم هنگام استفاده از مدل Claude اعتبار OpenAI مصرف می‌کند؟

مدل Claude مدل توگذاری خاص خود را ندارد. بنابراین، فرآیند توگذاری و سایر تولیدات گفتگو مانند پیشنهادات سوال بعدی به طور پیش‌فرض از کلیدهای OpenAI استفاده می‌کنند. این به این معنی است که اعتبار OpenAI هنوز هم مصرف می‌شود. می‌توانید مدل‌های استنباط و توگذاری پیش‌فرض مختلفی را در **تنظیمات > ارائه دهنده مدل** تنظیم کنید.

### 14. آیا راهی برای کنترل استفاده بیشتر از داده‌های دانش به جای قابلیت‌های تولید خود مدل وجود دارد؟

اینکه آیا از پایگاه دانش استفاده شود یا خیر به توصیف دانش بستگی دارد. لطفاً توصیف دانش را تا حد امکان به وضوح بنویسید. برای اطلاعات بیشتر، به [مستندات](https://docs.dify.ai/advanced/datasets) مراجعه کنید.

### 15. چگونه می‌توان سند دانش آپلود شده در اکسل را بهتر بخش‌بندی کرد؟

سربرگ را در ردیف اول تنظیم کنید و محتوا را در هر ردیف بعدی نمایش دهید. هیچ تنظیم سربرگ اضافی یا محتوای جدول فرمت شده پیچیده نداشته باشید.

### 16. من قبلاً ChatGPT Plus را خریداری کرده‌ام، چرا هنوز نمی‌توانم از GPT4 در Dify استفاده کنم؟

ChatGPT Plus و API مدل GPT-4 OpenAI دو محصول جداگانه با قیمت‌گذاری جداگانه هستند. API مدل‌ها دارای ساختار قیمت‌گذاری خاص خود هستند، برای جزئیات به [مستندات قیمت‌گذاری OpenAI](https://openai.com/pricing) مراجعه کنید. برای دسترسی به API مدل GPT-4، باید برای یک دوره صورت حساب پرداخت کنید - فقط داشتن یک روش پرداخت در پرونده و دسترسی به GPT-3.5 از طریق ChatGPT Plus کافی نیست. برای جزئیات کامل در مورد به دست آوردن دسترسی به GPT-4، به [مستندات رسمی OpenAI](https://platform.openai.com/account/billing/overview) مراجعه کنید.

### 17. چگونه می‌توان مدل‌های توگذاری دیگر را اضافه کرد؟

Dify از استفاده از ارائه دهندگان ذکر شده به عنوان ارائه دهنده مدل توگذاری پشتیبانی می‌کند، فقط نوع `توگذاری` را در کادر پیکربندی انتخاب کنید.

* Azure
* LocalAI
* MiniMax
* OpenAI
* Replicate
* XInference

### 18. چگونه می‌توان برنامه‌ای را که خودم ایجاد کرده‌ام به عنوان یک الگوی برنامه تنظیم کرد؟

توانایی تنظیم برنامه‌ای را که خودم ایجاد کرده‌ام به عنوان یک الگو در حال حاضر پشتیبانی نمی‌شود. الگوهای موجود توسط Dify به طور رسمی برای مرجع کاربران نسخه ابری ارائه می‌شود. اگر از نسخه ابری استفاده می‌کنید، می‌توانید برنامه‌هایی را به فضای کاری خود اضافه کنید یا آن‌ها را سفارشی کنید تا پس از اصلاحات برنامه خود را بسازید. اگر از نسخه community  استفاده می‌کنید و به ایجاد الگوهای برنامه بیشتر برای تیم خود نیاز دارید، می‌توانید برای دریافت پشتیبانی فنی پولی با تیم تجاری ما مشورت کنید: [business@dify.ai](mailto:business@dify.ai)


### 19. 502 Bad Gateway

این مشکل به دلیل هدایت Nginx  سرویس به موقعیت اشتباه اتفاق می‌افتد. ابتدا مطمئن شوید که کانتینر در حال اجرا است، سپس دستور زیر را با امتیازات ریشه اجرا کنید:
```
docker ps -q | xargs -n 1 docker inspect --format '{{ .Name }}: {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}'
```
این دو خط را در خروجی پیدا کنید:
```
/docker-web-1: 172.19.0.5
/docker-api-1: 172.19.0.7
```
آدرس‌های IP در انتها را به خاطر بسپارید. سپس، مکانی را که کد منبع dify را در آن ذخیره کرده‌اید، باز کنید، dify/docker/nginx/conf.d را باز کنید، http://api:5001 را با http://172.19.0.7:5001 جایگزین کنید و http://web:3000 را با http://172.19.0.5:3000 جایگزین کنید. پس از آن، کانتینر Nginx را مجدداً راه‌اندازی کنید یا پیکربندی را دوباره بارگذاری کنید. 
این آدرس‌های IP ***مثال*** هستند، شما باید دستور را اجرا کنید تا آدرس IP خود را دریافت کنید، آن را مستقیماً پر نکنید. 
ممکن است هنگام راه‌اندازی مجدد کانتینرهای مرتبط، نیاز به پیکربندی مجدد بر اساس IP داشته باشید.


