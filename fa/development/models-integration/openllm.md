# اتصال به مدل‌های OpenLLM مستقر محلی

با [OpenLLM](https://github.com/bentoml/OpenLLM) می‌توانید استنتاج را با هر مدل زبان بزرگ منبع باز انجام دهید، آن را در ابر یا به صورت محلی مستقر کنید و برنامه‌های AI قدرتمندی بسازید.
و Dify از اتصال به قابلیت‌های استنتاج مدل زبان بزرگ OpenLLM که به صورت محلی مستقر شده‌اند، پشتیبانی می‌کند.

## استقرار مدل OpenLLM
### راه‌اندازی OpenLLM

هر سرور OpenLLM می‌تواند یک مدل را مستقر کند، و شما می‌توانید آن را به روش زیر مستقر کنید:

```bash
docker run --rm -it -p 3333:3000 ghcr.io/bentoml/openllm start facebook/opt-1.3b --backend pt
```

> نکته: استفاده از مدل `facebook/opt-1.3b` در اینجا فقط برای نمایش است، و ممکن است اثر خوبی نداشته باشد. لطفا با توجه به شرایط واقعی، مدل مناسب را انتخاب کنید. برای مدل‌های بیشتر، لطفا به: [لیست مدل‌های پشتیبانی شده](https://github.com/bentoml/OpenLLM#-supported-models) مراجعه کنید.

پس از استقرار مدل، از مدل متصل شده در Dify استفاده کنید.

   زیر `Settings > Model Providers > OpenLLM` را پر کنید:

   - نام مدل: `facebook/opt-1.3b`
   - آدرس URL سرور: `http://<Machine_IP>:3333` با آدرس IP ماشین خود جایگزین کنید

   روی "Save" کلیک کنید و مدل را می توان در برنامه استفاده کرد.

این راهنما فقط برای اتصال سریع به عنوان مثال است. برای ویژگی‌ها و اطلاعات بیشتر در مورد استفاده از OpenLLM، لطفا به: [OpenLLM](https://github.com/bentoml/OpenLLM) مراجعه کنید. 
